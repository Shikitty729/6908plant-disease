{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37fcef7c-53cd-4f11-83e4-6197e7206191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\anaconda\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in d:\\anaconda\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\anaconda\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\anaconda\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\anaconda\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8800722a-c625-4767-9cd0-21cc2bed6a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "data_dir = \"F:/PlantVillage\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir = os.path.join(data_dir, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5998caa-6f61-45d3-b42e-3c607f831120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy', 'background']\n"
     ]
    }
   ],
   "source": [
    "# ÂõæÂÉèÈ¢ÑÂ§ÑÁêÜ\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Âä†ËΩΩÊï∞ÊçÆ\n",
    "image_datasets = {\n",
    "    \"train\": datasets.ImageFolder(train_dir, data_transforms[\"train\"]),\n",
    "    \"val\": datasets.ImageFolder(val_dir, data_transforms[\"val\"])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(image_datasets[\"train\"], batch_size=32, shuffle=True),\n",
    "    \"val\": DataLoader(image_datasets[\"val\"], batch_size=32, shuffle=False)\n",
    "}\n",
    "\n",
    "class_names = image_datasets[\"train\"].classes\n",
    "print(\"Classes:\", class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1dc978e-c4e0-4445-8fdc-e58d2e6fbcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to C:\\Users\\Nika/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-b0353104.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.6M/13.6M [00:00<00:00, 21.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# ÂÜªÁªìÁâπÂæÅÊèêÂèñÂ±Ç\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# ÊõøÊç¢ÂàÜÁ±ªÂ§¥ÔºàÊ†πÊçÆÁ±ªÂà´Êï∞ÈáèÔºâ\n",
    "num_classes = len(class_names)\n",
    "model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ced4d92-0fb4-4033-ac0e-c24a36011766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training images: 44016\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_dir = \"F:/PlantVillage/train\"  \n",
    "total_images = 0\n",
    "\n",
    "for root, dirs, files in os.walk(train_dir):\n",
    "    image_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    total_images += len(image_files)\n",
    "\n",
    "print(f\"Total training images: {total_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45ad4ed6-a158-485a-b147-925153a7bc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 188.5012, Accuracy: 95.63%\n",
      "Epoch 2/10\n",
      "Train Loss: 176.3813, Accuracy: 95.71%\n",
      "Epoch 3/10\n",
      "Train Loss: 169.9178, Accuracy: 95.76%\n",
      "Epoch 4/10\n",
      "Train Loss: 164.9595, Accuracy: 95.93%\n",
      "Epoch 5/10\n",
      "Train Loss: 159.7064, Accuracy: 96.15%\n",
      "Epoch 6/10\n",
      "Train Loss: 154.5710, Accuracy: 96.29%\n",
      "Epoch 7/10\n",
      "Train Loss: 153.0787, Accuracy: 96.25%\n",
      "Epoch 8/10\n",
      "Train Loss: 149.6499, Accuracy: 96.34%\n",
      "Epoch 9/10\n",
      "Train Loss: 146.6369, Accuracy: 96.45%\n",
      "Epoch 10/10\n",
      "Train Loss: 147.5239, Accuracy: 96.44%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in dataloaders[\"train\"]:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Train Loss: {running_loss:.4f}, Accuracy: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfa08905-d9a9-47df-9f5f-ff7791392be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mobilenetv2_epoch10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d5f86d5-177d-4e17-8b86-7d4c04002c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "# ÈáçÊñ∞ÂàõÂª∫ MobileNetV2 Ê®°Âûã\n",
    "model = models.mobilenet_v2(pretrained=False)\n",
    "\n",
    "num_classes = 39  \n",
    "model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
    "\n",
    "# Âä†ËΩΩÂèÇÊï∞\n",
    "model.load_state_dict(torch.load(\"mobilenetv2_epoch10.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"‚úÖ Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdd33d5b-4c05-4788-9eb7-366b42239f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Ë∑ØÂæÑËÆæÁΩÆÔºàÊõøÊç¢‰∏∫‰Ω†Ëá™Â∑±ÁöÑÈ™åËØÅÈõÜË∑ØÂæÑÔºâ\n",
    "val_dir = \"F:/PlantVillage/val\"\n",
    "\n",
    "# ÂÆö‰πâÈ™åËØÅÈõÜÁöÑÂõæÂÉèÈ¢ÑÂ§ÑÁêÜ\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Âä†ËΩΩÈ™åËØÅÈõÜ\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=val_transform)\n",
    "\n",
    "# ÂàõÂª∫ dataloader\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "dataloaders = {\"val\": val_loader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4820066-104a-4afe-9e88-714e24d2c0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Validation Accuracy: 97.01%\n",
      "üìâ Validation Loss: 0.0898\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloaders[\"val\"]:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)       # ‚úÖ ËÆ°ÁÆóÈ™åËØÅÊçüÂ§±\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "val_acc = 100 * correct / total\n",
    "avg_val_loss = val_loss / len(dataloaders[\"val\"])\n",
    "\n",
    "print(f\"‚úÖ Validation Accuracy: {val_acc:.2f}%\")\n",
    "print(f\"üìâ Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b567262-09f7-4aa0-bea9-e1d411f94411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#My training process went very smoothly, and the overall performance was excellent. From the training accuracy and loss, the model showed consistent convergence across all epochs. The training accuracy increased from 95.63% to 96.44%, while the loss steadily decreased, indicating that the optimizer and learning rate were well-tuned, and the model was effectively learning meaningful features.\n",
    "\n",
    "#At the same time, the validation accuracy reached 97.01%, with a very low validation loss of just 0.0898. There was no sign of overfitting, as the validation performance did not lag behind the training performance‚Äîin fact, the model performed slightly better on the validation set. This suggests that the model has strong generalization ability, and the validation set may contain cleaner or more representative examples that match the learned features well.\n",
    "\n",
    "#Given this training-validation behavior, I believe the number of training epochs was sufficient and that the model has already learned effectively. Further improvement might be possible by slightly increasing the number of epochs, applying light data augmentation, or exploring techniques like model distillation. However, based on the current results, I‚Äôm confident that the model is well-trained and ready for stable deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
