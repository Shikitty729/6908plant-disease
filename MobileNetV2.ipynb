{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37fcef7c-53cd-4f11-83e4-6197e7206191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\anaconda\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in d:\\anaconda\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\anaconda\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\anaconda\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\anaconda\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8800722a-c625-4767-9cd0-21cc2bed6a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "data_dir = \"F:/PlantVillage\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir = os.path.join(data_dir, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5998caa-6f61-45d3-b42e-3c607f831120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy', 'background']\n"
     ]
    }
   ],
   "source": [
    "# 图像预处理\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}\n",
    "\n",
    "# 加载数据\n",
    "image_datasets = {\n",
    "    \"train\": datasets.ImageFolder(train_dir, data_transforms[\"train\"]),\n",
    "    \"val\": datasets.ImageFolder(val_dir, data_transforms[\"val\"])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(image_datasets[\"train\"], batch_size=32, shuffle=True),\n",
    "    \"val\": DataLoader(image_datasets[\"val\"], batch_size=32, shuffle=False)\n",
    "}\n",
    "\n",
    "class_names = image_datasets[\"train\"].classes\n",
    "print(\"Classes:\", class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1dc978e-c4e0-4445-8fdc-e58d2e6fbcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to C:\\Users\\Nika/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-b0353104.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 13.6M/13.6M [00:00<00:00, 21.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# 冻结特征提取层\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 替换分类头（根据类别数量）\n",
    "num_classes = len(class_names)\n",
    "model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ced4d92-0fb4-4033-ac0e-c24a36011766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training images: 44016\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_dir = \"F:/PlantVillage/train\"  \n",
    "total_images = 0\n",
    "\n",
    "for root, dirs, files in os.walk(train_dir):\n",
    "    image_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    total_images += len(image_files)\n",
    "\n",
    "print(f\"Total training images: {total_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45ad4ed6-a158-485a-b147-925153a7bc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 188.5012, Accuracy: 95.63%\n",
      "Epoch 2/10\n",
      "Train Loss: 176.3813, Accuracy: 95.71%\n",
      "Epoch 3/10\n",
      "Train Loss: 169.9178, Accuracy: 95.76%\n",
      "Epoch 4/10\n",
      "Train Loss: 164.9595, Accuracy: 95.93%\n",
      "Epoch 5/10\n",
      "Train Loss: 159.7064, Accuracy: 96.15%\n",
      "Epoch 6/10\n",
      "Train Loss: 154.5710, Accuracy: 96.29%\n",
      "Epoch 7/10\n",
      "Train Loss: 153.0787, Accuracy: 96.25%\n",
      "Epoch 8/10\n",
      "Train Loss: 149.6499, Accuracy: 96.34%\n",
      "Epoch 9/10\n",
      "Train Loss: 146.6369, Accuracy: 96.45%\n",
      "Epoch 10/10\n",
      "Train Loss: 147.5239, Accuracy: 96.44%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in dataloaders[\"train\"]:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Train Loss: {running_loss:.4f}, Accuracy: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfa08905-d9a9-47df-9f5f-ff7791392be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mobilenetv2_epoch10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d5f86d5-177d-4e17-8b86-7d4c04002c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "# 重新创建 MobileNetV2 模型\n",
    "model = models.mobilenet_v2(pretrained=False)\n",
    "\n",
    "num_classes = 39  \n",
    "model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
    "\n",
    "# 加载参数\n",
    "model.load_state_dict(torch.load(\"mobilenetv2_epoch10.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"✅ Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdd33d5b-4c05-4788-9eb7-366b42239f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# 路径设置（替换为你自己的验证集路径）\n",
    "val_dir = \"F:/PlantVillage/val\"\n",
    "\n",
    "# 定义验证集的图像预处理\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# 加载验证集\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=val_transform)\n",
    "\n",
    "# 创建 dataloader\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "dataloaders = {\"val\": val_loader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4820066-104a-4afe-9e88-714e24d2c0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation Accuracy: 97.01%\n",
      "📉 Validation Loss: 0.0898\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloaders[\"val\"]:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)       # ✅ 计算验证损失\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "val_acc = 100 * correct / total\n",
    "avg_val_loss = val_loss / len(dataloaders[\"val\"])\n",
    "\n",
    "print(f\"✅ Validation Accuracy: {val_acc:.2f}%\")\n",
    "print(f\"📉 Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b567262-09f7-4aa0-bea9-e1d411f94411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#My training process went very smoothly, and the overall performance was excellent. From the training accuracy and loss, the model showed consistent convergence across all epochs. The training accuracy increased from 95.63% to 96.44%, while the loss steadily decreased, indicating that the optimizer and learning rate were well-tuned, and the model was effectively learning meaningful features.\n",
    "\n",
    "#At the same time, the validation accuracy reached 97.01%, with a very low validation loss of just 0.0898. There was no sign of overfitting, as the validation performance did not lag behind the training performance—in fact, the model performed slightly better on the validation set. This suggests that the model has strong generalization ability, and the validation set may contain cleaner or more representative examples that match the learned features well.\n",
    "\n",
    "#Given this training-validation behavior, I believe the number of training epochs was sufficient and that the model has already learned effectively. Further improvement might be possible by slightly increasing the number of epochs, applying light data augmentation, or exploring techniques like model distillation. However, based on the current results, I’m confident that the model is well-trained and ready for stable deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
